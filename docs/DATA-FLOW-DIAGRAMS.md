# ğŸ”„ Data Flow Diagrams - Haystack RAG System

Este documento apresenta as representaÃ§Ãµes visuais dos fluxos de dados no sistema Haystack RAG, complementando as descriÃ§Ãµes textuais disponÃ­veis no [README.md](../README.md).

> ğŸ“– **Para contexto tÃ©cnico**: [ARCHITECTURE.md](ARCHITECTURE.md)  
> ğŸ”Œ **Para integraÃ§Ã£o**: [API.md](API.md)  
> ğŸš€ **Para deploy**: [DEPLOYMENT.md](DEPLOYMENT.md)

## ğŸ“‹ Ãndice

- [Pipeline de Upload e Processamento](#-pipeline-de-upload-e-processamento)
- [Pipeline de Consulta RAG](#-pipeline-de-consulta-rag)
- [Fluxo de Cache e OtimizaÃ§Ã£o](#-fluxo-de-cache-e-otimizaÃ§Ã£o)
- [Arquitetura de Componentes](#-arquitetura-de-componentes)
- [Fluxos de Dados Detalhados](#-fluxos-de-dados-detalhados)

## ğŸ“¤ Pipeline de Upload e Processamento

### Fluxo Principal de Upload

```mermaid
graph TB
    %% Entrada de Dados
    User[ğŸ‘¤ UsuÃ¡rio] --> UI{Interface de Upload}
    UI --> Streamlit[ğŸ–¥ï¸ Streamlit Frontend<br/>:8501]
    UI --> API[ğŸ”Œ Direct API<br/>:8000/upload]

    %% Processamento de Upload
    Streamlit --> Upload[ğŸ“¤ Upload Handler]
    API --> Upload

    Upload --> Validate[âœ… ValidaÃ§Ã£o<br/>Formato & Tamanho]
    Validate --> Extract[ğŸ“„ ExtraÃ§Ã£o de Texto<br/>PyPDF, python-docx]

    %% Processamento de Texto
    Extract --> Split[âœ‚ï¸ Text Splitting<br/>Chunk Size: 1000<br/>Overlap: 200]
    Split --> Embed[ğŸ§  Embedding Generation<br/>OpenAI text-embedding-3-small]

    %% Cache e PersistÃªncia
    Embed --> Cache[ğŸ’¾ Redis Cache<br/>Embeddings + Metadata]
    Cache --> Store[ğŸ—„ï¸ Pinecone Vector Store<br/>IndexaÃ§Ã£o por Namespace]

    %% Resposta
    Store --> Response[âœ… Upload Response<br/>Document ID + Stats]
    Response --> User

    %% Estilo dos nÃ³s
    classDef user fill:#e1f5fe
    classDef interface fill:#f3e5f5
    classDef processing fill:#e8f5e8
    classDef storage fill:#fff3e0
    classDef response fill:#e0f2f1

    class User user
    class Streamlit,API interface
    class Upload,Validate,Extract,Split,Embed processing
    class Cache,Store storage
    class Response response
```

### Detalhamento do Processamento

```mermaid
sequenceDiagram
    participant U as ğŸ‘¤ User
    participant S as ğŸ–¥ï¸ Streamlit
    participant A as ğŸ”Œ API Handler
    participant V as âœ… Validator
    participant E as ğŸ“„ Extractor
    participant T as âœ‚ï¸ Text Splitter
    participant M as ğŸ§  Embedder
    participant R as ğŸ’¾ Redis
    participant P as ğŸ—„ï¸ Pinecone

    U->>S: Upload arquivo (PDF/TXT/DOCX)
    S->>A: POST /upload + file + params

    A->>V: Validar arquivo
    V-->>A: âœ… ValidaÃ§Ã£o OK

    A->>E: Extrair texto do arquivo
    E-->>A: Texto extraÃ­do + metadata

    A->>T: Dividir em chunks
    T-->>A: Lista de chunks

    loop Para cada chunk
        A->>M: Gerar embedding
        M->>R: Cache embedding
        R-->>M: Cache hit/miss
        M-->>A: Vector embedding
    end

    A->>P: Indexar no namespace
    P-->>A: Index response

    A-->>S: Upload success + stats
    S-->>U: ConfirmaÃ§Ã£o + progress
```

## ğŸ” Pipeline de Consulta RAG

### Fluxo Principal de Consulta

```mermaid
graph TB
    %% Entrada da Query
    User[ğŸ‘¤ UsuÃ¡rio] --> UI{Interface de Consulta}
    UI --> OpenWebUI[ğŸ¤– OpenWebUI<br/>:3000]
    UI --> DirectAPI[ğŸ”Œ Direct API<br/>:8000/rag/query]

    %% Processamento da Query
    OpenWebUI --> RAG[ğŸ§  RAG Pipeline]
    DirectAPI --> RAG

    RAG --> QueryEmbed[ğŸ” Query Embedding<br/>OpenAI text-embedding-3-small]
    QueryEmbed --> CacheCheck{ğŸ’¾ Cache Check<br/>Redis}

    %% Cache Hit Path
    CacheCheck -->|Cache Hit| CachedResponse[âš¡ Cached Response]
    CachedResponse --> FormatResponse[ğŸ“ Format Response]

    %% Cache Miss Path
    CacheCheck -->|Cache Miss| VectorSearch[ğŸ” Vector Search<br/>Pinecone Similarity]
    VectorSearch --> RetrieveDocs[ğŸ“š Retrieve Documents<br/>Top-K Filtering]

    RetrieveDocs --> PromptBuild[ğŸ“ Prompt Builder<br/>Context + Question]
    PromptBuild --> LLM[ğŸ¤– LLM Generation<br/>GPT-4o / Claude]

    LLM --> CacheStore[ğŸ’¾ Cache Store<br/>Redis TTL]
    CacheStore --> FormatResponse

    %% Resposta Final
    FormatResponse --> ResponseData[ğŸ“‹ Structured Response<br/>Answer + Sources + Metadata]
    ResponseData --> User

    %% Estilos
    classDef user fill:#e1f5fe
    classDef interface fill:#f3e5f5
    classDef processing fill:#e8f5e8
    classDef storage fill:#fff3e0
    classDef cache fill:#e3f2fd
    classDef response fill:#e0f2f1

    class User user
    class OpenWebUI,DirectAPI interface
    class RAG,QueryEmbed,VectorSearch,RetrieveDocs,PromptBuild,LLM processing
    class CacheCheck,CacheStore storage
    class CachedResponse cache
    class FormatResponse,ResponseData response
```

### Fluxo Detalhado de RAG

```mermaid
sequenceDiagram
    participant U as ğŸ‘¤ User
    participant O as ğŸ¤– OpenWebUI
    participant R as ğŸ§  RAG Pipeline
    participant E as ğŸ” Embedder
    participant C as ğŸ’¾ Redis Cache
    participant P as ğŸ—„ï¸ Pinecone
    participant L as ğŸ¤– LLM (GPT-4o)

    U->>O: "Como configurar SSL?"
    O->>R: POST /rag/query + params

    R->>E: Embed query
    E-->>R: Query vector [1536 dims]

    R->>C: Check cache (query hash)
    alt Cache Hit
        C-->>R: Cached response
        R-->>O: âš¡ Fast response
    else Cache Miss
        R->>P: Vector similarity search
        P-->>R: Top-K documents + scores

        R->>R: Build context prompt
        R->>L: Generate answer
        L-->>R: LLM response + metadata

        R->>C: Cache response (TTL: 1h)
        R-->>O: ğŸ“‹ Complete response
    end

    O-->>U: Answer + sources + metadata
```

## ğŸ’¾ Fluxo de Cache e OtimizaÃ§Ã£o

### EstratÃ©gia de Cache Multi-Camada

```mermaid
graph LR
    %% Entrada
    Request[ğŸ“¥ Request] --> L1{L1: Memory Cache<br/>Application Level}

    %% Layer 1 - Application Memory
    L1 -->|Hit| FastResponse[âš¡ Instant Response<br/>< 1ms]
    L1 -->|Miss| L2{L2: Redis Cache<br/>Distributed}

    %% Layer 2 - Redis
    L2 -->|Hit| MediumResponse[ğŸ”¥ Fast Response<br/>< 10ms]
    L2 -->|Miss| L3{L3: Embedding Cache<br/>Vector Store}

    %% Layer 3 - Vector Cache
    L3 -->|Hit| ComputeResponse[ğŸ”„ Compute Response<br/>< 100ms]
    L3 -->|Miss| FullCompute[ğŸ§  Full Processing<br/>1-5s]

    %% Cache Storage
    FullCompute --> UpdateCaches[ğŸ“ Update All Caches]
    UpdateCaches --> L1
    UpdateCaches --> L2
    UpdateCaches --> L3

    %% Cache Types
    subgraph "Cache Categories"
        CacheEmbed[ğŸ§  Embeddings<br/>TTL: 24h]
        CacheQuery[ğŸ” Query Results<br/>TTL: 1h]
        CacheDoc[ğŸ“„ Document Metadata<br/>TTL: 12h]
    end

    %% Styles
    classDef cache fill:#e3f2fd
    classDef fast fill:#c8e6c9
    classDef medium fill:#fff9c4
    classDef slow fill:#ffcdd2

    class L1,L2,L3,CacheEmbed,CacheQuery,CacheDoc cache
    class FastResponse,MediumResponse fast
    class ComputeResponse medium
    class FullCompute slow
```

### Cache Performance Metrics

```mermaid
pie title Cache Hit Rates by Type
    "Embedding Cache" : 85
    "Query Cache" : 65
    "Document Cache" : 90
    "Miss (Full Compute)" : 15
```

## ğŸ—ï¸ Arquitetura de Componentes

### VisÃ£o Geral da Arquitetura

```mermaid
graph TB
    %% Frontend Layer
    subgraph "ğŸ–¥ï¸ Frontend Layer"
        OpenWebUI[ğŸ¤– OpenWebUI<br/>Chat Interface<br/>:3000]
        Streamlit[ğŸ“¤ Streamlit<br/>Upload Interface<br/>:8501]
        WebApp[ğŸŒ Custom Web App<br/>Optional<br/>:8080]
    end

    %% API Gateway Layer
    subgraph "ğŸ”Œ API Gateway Layer"
        Nginx[ğŸ”„ Nginx Proxy<br/>Load Balancer<br/>:80/443]
        DirectAPI[âš¡ Direct API<br/>FastAPI<br/>:8000]
        HayAPI[ğŸ§  Haystack API<br/>Hayhooks<br/>:1416]
    end

    %% Business Logic Layer
    subgraph "ğŸ§  Business Logic Layer"
        RAGPipeline[ğŸ” RAG Pipeline<br/>Haystack Components]
        ChatPipeline[ğŸ’¬ Chat Pipeline<br/>Direct LLM]
        UploadHandler[ğŸ“¤ Upload Handler<br/>Document Processing]
        CacheManager[ğŸ’¾ Cache Manager<br/>Redis Operations]
    end

    %% AI Services Layer
    subgraph "ğŸ¤– AI Services Layer"
        OpenAI[ğŸ§  OpenAI<br/>GPT-4o + Embeddings]
        Anthropic[ğŸ¤– Anthropic<br/>Claude Models]
        LocalLLM[ğŸ  Local LLM<br/>Optional Ollama]
    end

    %% Data Layer
    subgraph "ğŸ—„ï¸ Data Layer"
        Pinecone[ğŸ“Š Pinecone<br/>Vector Database]
        Redis[ğŸ’¾ Redis<br/>Cache + Sessions]
        FileSystem[ğŸ“ File System<br/>Temp Storage]
    end

    %% Monitoring Layer
    subgraph "ğŸ“Š Monitoring Layer"
        Prometheus[ğŸ“ˆ Prometheus<br/>Metrics Collection]
        Grafana[ğŸ“Š Grafana<br/>Dashboards]
        Logs[ğŸ“‹ Centralized Logs<br/>ELK Stack]
    end

    %% Connections
    OpenWebUI <--> Nginx
    Streamlit <--> Nginx
    WebApp <--> Nginx

    Nginx <--> DirectAPI
    Nginx <--> HayAPI

    DirectAPI <--> RAGPipeline
    DirectAPI <--> ChatPipeline
    DirectAPI <--> UploadHandler
    HayAPI <--> RAGPipeline

    RAGPipeline <--> CacheManager
    ChatPipeline <--> CacheManager
    UploadHandler <--> CacheManager

    RAGPipeline <--> OpenAI
    ChatPipeline <--> OpenAI
    RAGPipeline <--> Anthropic
    ChatPipeline <--> LocalLLM

    RAGPipeline <--> Pinecone
    UploadHandler <--> Pinecone
    CacheManager <--> Redis
    UploadHandler <--> FileSystem

    DirectAPI <--> Prometheus
    HayAPI <--> Prometheus
    Prometheus <--> Grafana

    %% Styles
    classDef frontend fill:#e1f5fe
    classDef api fill:#f3e5f5
    classDef logic fill:#e8f5e8
    classDef ai fill:#fff3e0
    classDef data fill:#fce4ec
    classDef monitor fill:#f1f8e9

    class OpenWebUI,Streamlit,WebApp frontend
    class Nginx,DirectAPI,HayAPI api
    class RAGPipeline,ChatPipeline,UploadHandler,CacheManager logic
    class OpenAI,Anthropic,LocalLLM ai
    class Pinecone,Redis,FileSystem data
    class Prometheus,Grafana,Logs monitor
```

## ğŸ“Š Fluxos de Dados Detalhados

### MÃ©tricas de Performance

```mermaid
gantt
    title Performance Timeline por OperaÃ§Ã£o
    dateFormat X
    axisFormat %s

    section Upload Process
    File Validation     :0, 100
    Text Extraction     :100, 500
    Text Chunking       :500, 700
    Embedding Generation:700, 2000
    Vector Indexing     :2000, 2500
    Cache Storage       :2500, 2600

    section RAG Query
    Query Embedding     :0, 200
    Cache Check         :200, 210
    Vector Search       :210, 500
    Context Building    :500, 520
    LLM Generation      :520, 1500
    Response Formatting :1500, 1520
```

### Data Flow Metrics

| OperaÃ§Ã£o               | LatÃªncia MÃ©dia | Throughput    | Cache Hit Rate |
| ---------------------- | -------------- | ------------- | -------------- |
| **Upload**             | 2.5s           | 10 docs/min   | N/A            |
| **Query (Cache Hit)**  | 50ms           | 100 queries/s | 75%            |
| **Query (Cache Miss)** | 1.2s           | 20 queries/s  | 25%            |
| **Embedding**          | 200ms          | 50 vectors/s  | 85%            |

### Volume de Dados

```mermaid
graph LR
    subgraph "ğŸ“ˆ Volume Metrics"
        Docs[ğŸ“„ Documents<br/>~1000 docs]
        Chunks[âœ‚ï¸ Chunks<br/>~25,000 chunks]
        Vectors[ğŸ§  Vectors<br/>~25,000 x 1536 dims]
        Cache[ğŸ’¾ Cache Entries<br/>~5,000 queries]
    end

    Docs --> Chunks
    Chunks --> Vectors
    Vectors --> Cache

    subgraph "ğŸ’¾ Storage Requirements"
        VectorSize[Vector Storage<br/>~150MB]
        CacheSize[Cache Storage<br/>~50MB]
        MetaSize[Metadata<br/>~10MB]
        TotalSize[Total: ~210MB]
    end
```

## ğŸ”„ OtimizaÃ§Ãµes Implementadas

### Performance Optimizations

1. **ğŸš€ Connection Pooling**

   ```python
   # Pinecone connection pool
   pinecone_pool = ConnectionPool(max_connections=10)

   # Redis connection pool
   redis_pool = redis.ConnectionPool(max_connections=20)
   ```

2. **âš¡ Async Processing**

   ```python
   # Parallel embedding generation
   async def process_chunks_parallel(chunks):
       tasks = [embed_chunk(chunk) for chunk in chunks]
       return await asyncio.gather(*tasks)
   ```

3. **ğŸ’¾ Smart Caching**

   ```python
   # Multi-level cache strategy
   @cache_with_ttl(ttl=3600)  # 1 hour
   async def cached_rag_query(query_hash):
       return await rag_pipeline.run(query)
   ```

4. **ğŸ“Š Batch Operations**
   ```python
   # Batch vector upserts
   batch_size = 100
   for i in range(0, len(vectors), batch_size):
       batch = vectors[i:i+batch_size]
       await pinecone_index.upsert(batch)
   ```

---

## ğŸ¯ PrÃ³ximos Passos para OtimizaÃ§Ã£o

1. **ğŸ”„ Streaming Responses**: Implementar streaming para respostas longas
2. **ğŸ“Š Monitoring AvanÃ§ado**: Adicionar mÃ©tricas detalhadas de performance
3. **ğŸ§  Semantic Caching**: Cache baseado em similaridade semÃ¢ntica
4. **ğŸ”§ Auto-scaling**: Implementar scaling automÃ¡tico baseado em carga

---

**ğŸ“… Ãšltima AtualizaÃ§Ã£o**: Janeiro 2024  
**ğŸ”„ PrÃ³xima RevisÃ£o**: Fevereiro 2024  
**ğŸ“Š MÃ©tricas Baseadas**: Sistema em produÃ§Ã£o com ~1000 documentos
