# üîß Troubleshooting & Monitoring Guide - Haystack RAG System

Este guia fornece solu√ß√µes detalhadas para problemas comuns, an√°lise de logs, otimiza√ß√£o de performance e monitoramento do sistema Haystack RAG.

> üìñ **Para setup inicial**: [README.md](../README.md)  
> üöÄ **Para deploy avan√ßado**: [DEPLOYMENT.md](DEPLOYMENT.md)  
> üîå **Para APIs**: [API.md](API.md)

## üìã √çndice

- [Health Checks e Diagn√≥sticos](#-health-checks-e-diagn√≥sticos)
- [Problemas Comuns e Solu√ß√µes](#-problemas-comuns-e-solu√ß√µes)
- [An√°lise de Logs](#-an√°lise-de-logs)
- [Monitoramento e Performance](#-monitoramento-e-performance)
- [Otimiza√ß√£o de Sistema](#-otimiza√ß√£o-de-sistema)
- [Scripts de Diagn√≥stico](#-scripts-de-diagn√≥stico)

## ü©∫ Health Checks e Diagn√≥sticos

### Verifica√ß√£o R√°pida do Sistema

```bash
#!/bin/bash
# Quick system health check

echo "üîç Verificando status dos servi√ßos..."

# 1. Docker containers
echo "üì¶ Docker Containers:"
docker-compose ps

# 2. API Health checks
echo "üîå API Endpoints:"
curl -s http://localhost:8000/health | jq '.' || echo "‚ùå API Principal indispon√≠vel"
curl -s http://localhost:1416/status | jq '.' || echo "‚ùå Hayhooks indispon√≠vel"
curl -s http://localhost:3000/health || echo "‚ùå OpenWebUI indispon√≠vel"

# 3. Database connections
echo "üóÑÔ∏è Database Connections:"
docker-compose exec redis redis-cli ping || echo "‚ùå Redis indispon√≠vel"

# 4. External services
echo "üåê External Services:"
curl -s -H "Authorization: Bearer $OPENAI_API_KEY" \
  https://api.openai.com/v1/models >/dev/null && echo "‚úÖ OpenAI API" || echo "‚ùå OpenAI API"

# 5. Disk space
echo "üíæ Disk Space:"
df -h | grep -E "/$|/var"
```

### Health Check Endpoints

| Servi√ßo           | URL                   | Status Esperado                                                        |
| ----------------- | --------------------- | ---------------------------------------------------------------------- |
| **API Principal** | `GET /health`         | `{"status": "healthy", "redis": "connected", "pinecone": "connected"}` |
| **Hayhooks**      | `GET /status`         | `{"status": "ready", "pipelines": [...]}`                              |
| **OpenWebUI**     | `GET /health`         | `200 OK`                                                               |
| **Redis**         | `redis-cli ping`      | `PONG`                                                                 |
| **Streamlit**     | `GET /_stcore/health` | `200 OK`                                                               |

### Comandos de Diagn√≥stico Detalhado

```bash
# Status completo do sistema
curl http://localhost:8000/health | jq '.'

# Estat√≠sticas do cache
curl http://localhost:8000/cache/stats | jq '.'

# Informa√ß√µes dos pipelines
curl http://localhost:1416/status | jq '.'

# Modelos dispon√≠veis
curl http://localhost:8000/api/models | jq '.'

# Namespaces ativos
curl http://localhost:8000/api/namespaces | jq '.'

# M√©tricas de documentos
curl http://localhost:8000/api/documents | jq '.'
```

## üö® Problemas Comuns e Solu√ß√µes

### 1. **Erro de API Key**

#### Sintomas:

```
HTTP 401: {"detail": "Invalid API key"}
Authentication failed for OpenAI API
```

#### Diagn√≥stico:

```bash
# Verificar vari√°veis de ambiente
docker-compose exec hayhooks env | grep -E "(OPENAI|PINECONE|ANTHROPIC)_API_KEY"

# Testar API key diretamente
curl -H "Authorization: Bearer $OPENAI_API_KEY" \
  https://api.openai.com/v1/models
```

#### Solu√ß√µes:

1. **Verificar .env file**:

   ```bash
   cat .env | grep API_KEY
   ```

2. **Recarregar configura√ß√µes**:

   ```bash
   docker-compose down
   docker-compose up -d
   ```

3. **Validar formato da API key**:
   - OpenAI: `sk-...` (51 chars)
   - Pinecone: UUID format
   - Anthropic: `sk-ant-...`

### 2. **Pinecone Connection Issues**

#### Sintomas:

```
PineconeApiException: Index 'haystack-docs' not found
Connection timeout to Pinecone
```

#### Diagn√≥stico:

```bash
# Testar conex√£o Pinecone
curl http://localhost:8000/rag/info

# Verificar configura√ß√£o
docker-compose exec hayhooks env | grep PINECONE
```

#### Solu√ß√µes:

1. **Verificar configura√ß√£o**:

   ```bash
   # Verificar se o √≠ndice existe
   python3 -c "
   import pinecone
   pinecone.init(api_key='$PINECONE_API_KEY', environment='$PINECONE_ENVIRONMENT')
   print(pinecone.list_indexes())
   "
   ```

2. **Criar √≠ndice se necess√°rio**:

   ```python
   import pinecone
   pinecone.init(api_key="your-api-key", environment="your-env")

   # Criar √≠ndice
   pinecone.create_index(
       name="haystack-docs",
       dimension=1536,  # OpenAI embeddings
       metric="cosine"
   )
   ```

3. **Verificar regi√£o/ambiente**:
   ```bash
   # Ambientes comuns: gcp-starter, us-east-1-aws, etc.
   echo $PINECONE_ENVIRONMENT
   ```

### 3. **Redis Cache Problems**

#### Sintomas:

```
redis.exceptions.ConnectionError: Error connecting to Redis
Cache miss for all queries
Slow response times
```

#### Diagn√≥stico:

```bash
# Testar conex√£o Redis
docker-compose exec redis redis-cli ping

# Verificar uso de mem√≥ria
docker-compose exec redis redis-cli info memory

# Verificar estat√≠sticas
curl http://localhost:8000/cache/stats
```

#### Solu√ß√µes:

1. **Restart Redis**:

   ```bash
   docker-compose restart redis
   ```

2. **Limpar cache**:

   ```bash
   docker-compose exec redis redis-cli flushall
   ```

3. **Verificar configura√ß√£o de mem√≥ria**:
   ```bash
   # Aumentar mem√≥ria se necess√°rio
   docker-compose exec redis redis-cli config set maxmemory 256mb
   ```

### 4. **OpenWebUI n√£o carrega**

#### Sintomas:

```
HTTP 500: Internal Server Error
Connection refused on port 3000
Login page n√£o aparece
```

#### Diagn√≥stico:

```bash
# Verificar logs
docker-compose logs open-webui

# Verificar processo
docker-compose exec open-webui ps aux

# Testar endpoint
curl http://localhost:3000/health
```

#### Solu√ß√µes:

1. **Restart servi√ßo**:

   ```bash
   docker-compose restart open-webui
   ```

2. **Rebuild se necess√°rio**:

   ```bash
   docker-compose build open-webui
   docker-compose up -d open-webui
   ```

3. **Verificar vari√°veis de ambiente**:
   ```bash
   docker-compose exec open-webui env | grep -E "(SECRET_KEY|WEBUI)"
   ```

### 5. **Upload de Documentos Falha**

#### Sintomas:

```
HTTP 413: Request Entity Too Large
Upload processing timeout
Files not appearing in vector store
```

#### Diagn√≥stico:

```bash
# Verificar logs do hayhooks
docker-compose logs hayhooks | grep -i upload

# Testar upload simples
curl -X POST "http://localhost:8000/upload" \
  -F "file=@small-test.txt"

# Verificar namespaces
curl http://localhost:8000/api/namespaces
```

#### Solu√ß√µes:

1. **Aumentar limites de upload**:

   ```yaml
   # Em docker-compose.yml
   hayhooks:
     environment:
       - MAX_UPLOAD_SIZE=100MB
   ```

2. **Verificar formatos suportados**:

   ```bash
   # Formatos: PDF, TXT, DOCX, MD, CSV, JSON
   file your-document.pdf
   ```

3. **Upload por partes**:
   ```bash
   # Dividir documentos grandes
   split -l 1000 large-file.txt chunk_
   ```

### 6. **Performance Issues**

#### Sintomas:

```
Query response > 10 seconds
High CPU usage
Memory exhaustion
```

#### Diagn√≥stico:

```bash
# Monitorar recursos
docker stats

# Verificar cache hit rate
curl http://localhost:8000/cache/stats | jq '.hit_rate'

# Verificar embedding cache
curl http://localhost:8000/api/embeddings/stats
```

#### Solu√ß√µes:

1. **Otimizar cache**:

   ```bash
   # Aumentar TTL do cache
   # Configurar em hayhooks/cache_manager.py
   CACHE_TTL = 3600  # 1 hora
   ```

2. **Reduzir chunk size**:

   ```python
   # Em pipelines/rag_pipeline.py
   chunk_size = 500  # Reduzir de 1000
   chunk_overlap = 100  # Reduzir overlap
   ```

3. **Connection pooling**:
   ```python
   # Configurar pool de conex√µes
   redis_pool = redis.ConnectionPool(max_connections=20)
   ```

## üìä An√°lise de Logs

### Estrutura de Logs

```bash
# Logs em tempo real
docker-compose logs -f

# Logs espec√≠ficos por servi√ßo
docker-compose logs hayhooks      # Backend principal
docker-compose logs open-webui    # Interface web
docker-compose logs redis         # Cache
docker-compose logs streamlit-upload  # Upload frontend
```

### Patterns de Log Importantes

#### 1. **Logs de Sucesso**

```
INFO: Application startup complete
INFO: Document uploaded successfully: doc_id=abc123
INFO: RAG query processed in 1.2s
INFO: Cache hit for query: query_hash=def456
```

#### 2. **Logs de Erro**

```
ERROR: Failed to connect to Pinecone: [details]
ERROR: OpenAI API rate limit exceeded
ERROR: Redis connection timeout
WARNING: Large document size: 15MB
```

#### 3. **Performance Logs**

```
INFO: Query embedding took 0.2s
INFO: Vector search took 0.8s
INFO: LLM generation took 2.1s
INFO: Total query time: 3.1s
```

### Log Analysis Scripts

```bash
#!/bin/bash
# analyze-logs.sh

LOG_FILE="logs/hayhooks.log"

echo "üìä Log Analysis Report"
echo "======================"

# Error rate
echo "üö® Error Rate:"
ERROR_COUNT=$(grep -c "ERROR" $LOG_FILE)
TOTAL_LINES=$(wc -l < $LOG_FILE)
echo "   Errors: $ERROR_COUNT / $TOTAL_LINES lines"

# Performance metrics
echo "‚è±Ô∏è Average Response Times:"
grep "Total query time" $LOG_FILE | awk '{print $NF}' | \
  sed 's/s$//' | awk '{sum+=$1; count++} END {print "   Query: " sum/count "s"}'

# Most common errors
echo "üîç Top Errors:"
grep "ERROR" $LOG_FILE | cut -d':' -f3- | sort | uniq -c | sort -nr | head -5

# Cache performance
echo "üíæ Cache Performance:"
grep "Cache hit" $LOG_FILE | wc -l | awk '{print "   Cache hits: " $1}'
grep "Cache miss" $LOG_FILE | wc -l | awk '{print "   Cache misses: " $1}'
```

## üìà Monitoramento e Performance

### M√©tricas Chave

#### 1. **Response Times**

```bash
# Monitorar lat√™ncia das APIs
while true; do
  time curl -s http://localhost:8000/health >/dev/null
  sleep 5
done
```

#### 2. **Resource Usage**

```bash
# CPU e Mem√≥ria
docker stats --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}"

# Disk Usage
du -sh data/
df -h
```

#### 3. **Cache Metrics**

```bash
# Redis stats
docker-compose exec redis redis-cli info stats | grep -E "(hits|misses|hit_rate)"

# Application cache
curl http://localhost:8000/cache/stats | jq '{hit_rate, total_queries, cache_size}'
```

### Performance Monitoring Script

```bash
#!/bin/bash
# monitor-performance.sh

INTERVAL=60  # segundos
LOG_FILE="performance.log"

echo "Starting performance monitoring (interval: ${INTERVAL}s)"

while true; do
    TIMESTAMP=$(date '+%Y-%m-%d %H:%M:%S')

    # API Response time
    API_TIME=$(time (curl -s http://localhost:8000/health >/dev/null) 2>&1 | \
              grep real | awk '{print $2}')

    # Memory usage
    MEM_USAGE=$(docker stats --no-stream --format "{{.MemUsage}}" hayhooks | \
               head -1)

    # Cache hit rate
    HIT_RATE=$(curl -s http://localhost:8000/cache/stats | jq -r '.hit_rate // 0')

    # Log metrics
    echo "$TIMESTAMP,API:$API_TIME,MEM:$MEM_USAGE,CACHE:$HIT_RATE" >> $LOG_FILE

    sleep $INTERVAL
done
```

### Alertas e Thresholds

```bash
#!/bin/bash
# check-thresholds.sh

# Thresholds
MAX_RESPONSE_TIME=5.0    # segundos
MIN_CACHE_HIT_RATE=0.6   # 60%
MAX_MEMORY_MB=2048       # 2GB

# Check API response time
RESPONSE_TIME=$(curl -w "%{time_total}" -s http://localhost:8000/health >/dev/null)
if (( $(echo "$RESPONSE_TIME > $MAX_RESPONSE_TIME" | bc -l) )); then
    echo "üö® ALERT: API response time too high: ${RESPONSE_TIME}s"
fi

# Check cache hit rate
HIT_RATE=$(curl -s http://localhost:8000/cache/stats | jq -r '.hit_rate // 0')
if (( $(echo "$HIT_RATE < $MIN_CACHE_HIT_RATE" | bc -l) )); then
    echo "üö® ALERT: Cache hit rate too low: ${HIT_RATE}"
fi

# Check memory usage
MEM_MB=$(docker stats --no-stream --format "{{.MemUsage}}" hayhooks | \
         sed 's/MiB.*//' | sed 's/.* //')
if (( $(echo "$MEM_MB > $MAX_MEMORY_MB" | bc -l) )); then
    echo "üö® ALERT: Memory usage too high: ${MEM_MB}MB"
fi
```

## ‚ö° Otimiza√ß√£o de Sistema

### 1. **Cache Optimization**

```python
# cache_manager.py optimizations
class OptimizedCacheManager:
    def __init__(self):
        self.redis_pool = redis.ConnectionPool(
            max_connections=20,
            retry_on_timeout=True,
            socket_timeout=5
        )

        # Multi-level TTL
        self.ttl_config = {
            "embeddings": 86400,      # 24 horas
            "query_results": 3600,    # 1 hora
            "document_meta": 43200,   # 12 horas
        }

    async def smart_cache_key(self, content: str) -> str:
        """Generate semantic cache key"""
        # Use content hash + embeddings similarity
        content_hash = hashlib.md5(content.encode()).hexdigest()
        return f"smart_cache:{content_hash}"
```

### 2. **Database Optimization**

```python
# Pinecone optimization
class OptimizedPinecone:
    def __init__(self):
        self.batch_size = 100
        self.connection_pool = ConnectionPool(max_connections=10)

    async def batch_upsert(self, vectors: List[Dict]):
        """Batch upload for better performance"""
        for i in range(0, len(vectors), self.batch_size):
            batch = vectors[i:i + self.batch_size]
            await self.index.upsert(vectors=batch)
            await asyncio.sleep(0.1)  # Rate limiting
```

### 3. **Resource Management**

```yaml
# docker-compose.yml resource limits
services:
  hayhooks:
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: "1.0"
        reservations:
          memory: 1G
          cpus: "0.5"

  redis:
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.5"
    command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru
```

### 4. **Query Optimization**

```python
# Optimized RAG pipeline
class OptimizedRAGPipeline:
    def __init__(self):
        # Reduced embedding dimensions for faster search
        self.embedding_model = "text-embedding-3-small"  # 1536 dims vs 3072

        # Optimized retrieval
        self.top_k = 5  # Reduced from 10
        self.score_threshold = 0.7  # Higher threshold

        # Async processing
        self.enable_concurrent_embedding = True

    async def optimized_query(self, query: str):
        # Parallel processing
        tasks = [
            self.embed_query(query),
            self.check_cache(query),
            self.warm_up_llm()
        ]
        results = await asyncio.gather(*tasks)
        return await self.process_results(results)
```

## üõ†Ô∏è Scripts de Diagn√≥stico

### Script de Diagn√≥stico Completo

```bash
#!/bin/bash
# comprehensive-diagnostic.sh

echo "üîç HAYSTACK RAG SYSTEM - COMPREHENSIVE DIAGNOSTIC"
echo "=================================================="
date
echo ""

# Function to check service
check_service() {
    local name=$1
    local url=$2
    local expected=$3

    echo -n "Checking $name... "
    response=$(curl -s -w "%{http_code}" -o /dev/null "$url" 2>/dev/null)

    if [ "$response" = "$expected" ]; then
        echo "‚úÖ OK"
    else
        echo "‚ùå FAIL (HTTP $response)"
    fi
}

# 1. DOCKER ENVIRONMENT
echo "üê≥ DOCKER ENVIRONMENT"
echo "---------------------"
docker --version
docker-compose --version
echo ""

# 2. CONTAINER STATUS
echo "üì¶ CONTAINER STATUS"
echo "-------------------"
docker-compose ps
echo ""

# 3. SERVICE ENDPOINTS
echo "üîå SERVICE ENDPOINTS"
echo "--------------------"
check_service "Hayhooks API" "http://localhost:8000/health" "200"
check_service "Hayhooks Pipelines" "http://localhost:1416/status" "200"
check_service "OpenWebUI" "http://localhost:3000" "200"
check_service "Streamlit Upload" "http://localhost:8501/_stcore/health" "200"
echo ""

# 4. DATABASE CONNECTIONS
echo "üóÑÔ∏è DATABASE CONNECTIONS"
echo "------------------------"
echo -n "Redis... "
if docker-compose exec -T redis redis-cli ping >/dev/null 2>&1; then
    echo "‚úÖ Connected"
else
    echo "‚ùå Failed"
fi
echo ""

# 5. EXTERNAL APIS
echo "üåê EXTERNAL API KEYS"
echo "--------------------"
if [ -n "$OPENAI_API_KEY" ]; then
    echo -n "OpenAI API... "
    if curl -s -H "Authorization: Bearer $OPENAI_API_KEY" \
       https://api.openai.com/v1/models >/dev/null 2>&1; then
        echo "‚úÖ Valid"
    else
        echo "‚ùå Invalid"
    fi
else
    echo "‚ö†Ô∏è  OpenAI API Key not set"
fi

if [ -n "$PINECONE_API_KEY" ]; then
    echo "‚úÖ Pinecone API Key set"
else
    echo "‚ö†Ô∏è  Pinecone API Key not set"
fi
echo ""

# 6. SYSTEM RESOURCES
echo "üíª SYSTEM RESOURCES"
echo "-------------------"
echo "Memory usage:"
docker stats --no-stream --format "  {{.Container}}: {{.MemUsage}}"
echo ""
echo "Disk usage:"
df -h / | tail -1 | awk '{print "  Root: " $3 " used / " $2 " total (" $5 " full)"}'
echo ""

# 7. LOG ANALYSIS
echo "üìã RECENT ERRORS"
echo "----------------"
echo "Last 10 errors from logs:"
docker-compose logs --tail=100 | grep -i error | tail -10 | \
  sed 's/^/  /' || echo "  No recent errors found"
echo ""

# 8. PERFORMANCE METRICS
echo "üìä PERFORMANCE METRICS"
echo "----------------------"
if curl -s http://localhost:8000/cache/stats >/dev/null 2>&1; then
    echo "Cache statistics:"
    curl -s http://localhost:8000/cache/stats | jq -r '
      "  Hit rate: " + (.hit_rate // 0 | tostring) + "%",
      "  Total queries: " + (.total_queries // 0 | tostring),
      "  Cache size: " + (.cache_size // "unknown" | tostring)
    ' 2>/dev/null || echo "  Cache stats unavailable"
else
    echo "  Cache statistics unavailable"
fi
echo ""

# 9. RECOMMENDATIONS
echo "üí° RECOMMENDATIONS"
echo "------------------"

# Check for common issues
if ! docker-compose exec -T redis redis-cli ping >/dev/null 2>&1; then
    echo "  - Fix Redis connection"
fi

if [ -z "$OPENAI_API_KEY" ]; then
    echo "  - Set OPENAI_API_KEY in .env file"
fi

if [ -z "$PINECONE_API_KEY" ]; then
    echo "  - Set PINECONE_API_KEY in .env file"
fi

# Check response times
response_time=$(curl -w "%{time_total}" -s http://localhost:8000/health >/dev/null 2>&1)
if (( $(echo "$response_time > 2.0" | bc -l) 2>/dev/null )); then
    echo "  - API response time is slow (${response_time}s), consider optimization"
fi

echo ""
echo "üìÖ Diagnostic completed at $(date)"
echo "For detailed troubleshooting, see: docs/TROUBLESHOOTING.md"
```

### Performance Benchmark Script

```bash
#!/bin/bash
# benchmark-performance.sh

echo "üöÄ PERFORMANCE BENCHMARK"
echo "========================"

# Test upload performance
echo "üì§ Testing upload performance..."
time curl -X POST "http://localhost:8000/upload" \
  -F "file=@test-documents/sample.pdf" \
  -F "namespace=benchmark" >/dev/null 2>&1

# Test query performance
echo "üîç Testing query performance..."
for i in {1..10}; do
    time curl -X POST "http://localhost:8000/rag/query" \
      -H "Content-Type: application/json" \
      -d '{"query": "What is this document about?", "namespace": "benchmark"}' \
      >/dev/null 2>&1
done

# Test cache performance
echo "üíæ Testing cache performance..."
# First query (cache miss)
time curl -X POST "http://localhost:8000/rag/query" \
  -H "Content-Type: application/json" \
  -d '{"query": "Cache test query", "namespace": "benchmark"}' \
  >/dev/null 2>&1

# Second query (cache hit)
time curl -X POST "http://localhost:8000/rag/query" \
  -H "Content-Type: application/json" \
  -d '{"query": "Cache test query", "namespace": "benchmark"}' \
  >/dev/null 2>&1

echo "Benchmark completed!"
```

---

## üö® Emergency Procedures

### Quick Reset

```bash
#!/bin/bash
# emergency-reset.sh

echo "üö® EMERGENCY SYSTEM RESET"
echo "========================="

# Stop all services
docker-compose down

# Clear cache
docker-compose exec redis redis-cli flushall 2>/dev/null || true

# Clear logs
docker-compose logs > backup-logs-$(date +%Y%m%d-%H%M%S).log
docker system prune -f

# Restart services
docker-compose up -d

# Wait for services
sleep 30

# Health check
curl http://localhost:8000/health
```

### Backup Critical Data

```bash
#!/bin/bash
# backup-system.sh

BACKUP_DIR="backup-$(date +%Y%m%d-%H%M%S)"
mkdir -p $BACKUP_DIR

# Backup Redis data
docker-compose exec redis redis-cli save
docker cp $(docker-compose ps -q redis):/data/dump.rdb $BACKUP_DIR/

# Backup configuration
cp .env $BACKUP_DIR/
cp docker-compose.yml $BACKUP_DIR/

# Backup logs
docker-compose logs > $BACKUP_DIR/logs.txt

echo "Backup created in $BACKUP_DIR"
```

---

**üìÖ √öltima Atualiza√ß√£o**: Janeiro 2024  
**üîÑ Pr√≥xima Revis√£o**: Mensal  
**üë• Respons√°vel**: DevOps + Tech Lead

Para **issues espec√≠ficas** n√£o cobertas neste guia, consulte:

- üìñ [README.md](../README.md) para overview
- üöÄ [DEPLOYMENT.md](DEPLOYMENT.md) para configura√ß√£o
- üîå [API.md](API.md) para APIs
- üèóÔ∏è [ARCHITECTURE.md](ARCHITECTURE.md) para arquitetura
